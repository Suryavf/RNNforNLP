{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label   \n",
    "            \n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    tokenized = [porter.stem(w) for w in text]\n",
    "    return text\n",
    "\n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    for _ in range(size):\n",
    "        text, label = next(doc_stream)\n",
    "        docs.append(text)\n",
    "        y.append(label)\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    import re\n",
    "    REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\n)\")\n",
    "    REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    text = REPLACE_NO_SPACE.sub('', text.lower())\n",
    "    text = REPLACE_WITH_SPACE.sub(' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50000\n",
    "n_train   = 40000\n",
    "n_test    = n_samples - n_train\n",
    "n_vector  = 100\n",
    "n_tokens  = 180\n",
    "\n",
    "fdata  = 'shuffled_movie_data.csv'\n",
    "fmodel = \"word2vec.model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load word2vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "doc_stream = stream_docs(path=fdata)\n",
    "\n",
    "common_texts, sentiment = get_minibatch(doc_stream, size=n_samples)\n",
    "common_texts = [ preprocessing(common_texts[n]).split() for n in range(n_samples) ]\n",
    "    \n",
    "# Save/Read model\n",
    "if not os.path.isfile(fmodel):\n",
    "    model = Word2Vec(common_texts, size=n_vector, \n",
    "                     window=10, min_count=1, workers=4)\n",
    "    model.save(fmodel)\n",
    "\n",
    "else:\n",
    "    model = Word2Vec.load(fmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch  = 500\n",
    "n_batch  = 100\n",
    "n_hidden = 10\n",
    "n_vocab  = len(model.wv.vocab)\n",
    "dropout  = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence(b):\n",
    "    start = b* n_batch\n",
    "    \n",
    "    # Batch\n",
    "    X = np.zeros( [n_batch,n_tokens,n_vector] )\n",
    "    y = np.array(sentiment[start:start+n_batch]).reshape([1,n_batch])\n",
    "    \n",
    "    for n in range(n_batch):\n",
    "        # New sample\n",
    "        words = common_texts[start+n]; n_words = len(words); \n",
    "        if n_words > n_tokens: n_words = n_tokens;\n",
    "        \n",
    "        # Model\n",
    "        for w in range(n_words):\n",
    "            X[n][w] = model.wv[ words[w] ]\n",
    "        \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "rnn = Sequential()\n",
    "\n",
    "# Embeding\n",
    "rnn.add( Bidirectional( LSTM( n_hidden, input_shape=(n_vector,1), \n",
    "                              return_sequences=True,\n",
    "                              dropout=dropout,\n",
    "                              recurrent_dropout=dropout,\n",
    "                              kernel_initializer = 'he_normal',\n",
    "                                bias_initializer = 'he_normal') ) )\n",
    "rnn.add( Bidirectional( LSTM( n_hidden,\n",
    "                              return_sequences=True,\n",
    "                              dropout=dropout,\n",
    "                              recurrent_dropout=dropout,\n",
    "                              kernel_initializer = 'he_normal',\n",
    "                                bias_initializer = 'he_normal') ) )\n",
    "rnn.add( Bidirectional( LSTM( n_hidden, \n",
    "                              return_sequences=True,\n",
    "                              dropout=dropout,\n",
    "                              recurrent_dropout=dropout,\n",
    "                              kernel_initializer = 'he_normal',\n",
    "                                bias_initializer = 'he_normal') ) )\n",
    "rnn.add( TimeDistributed(Dense( 1,         activation = 'sigmoid'  ,\n",
    "                                   kernel_initializer = 'he_normal',\n",
    "                                     bias_initializer = 'he_normal' ) ) )\n",
    "rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
